import pyarrow.parquet as pq
import pyarrow as pa
import pandas as pd
import numpy as np
import os
from Partition import *


def enumerate_partitions(columns, n):
    # columns is a list of lists of values
    # eg columns = [["A", "B"],[[1,2],[2,3]]]
    # return a list of all possible partitions
    # eg [["A", [1,2]], ["A", [2,3]], ["B", [1,2]], ["B", [2,3]]
    if n == 1:
        return [[part] for part in columns[0]]
    else:
        partitions = []
        for part in columns[0]:
            for partition in enumerate_partitions(columns[1:], n-1):
                partitions.append([part] + partition)
        return partitions


def sample_gen(column_set, output_set, table, error_rate=0.01, partition_size=10):
    # column_set is a set of column names
    # table is name of parquet file to be read
    # error_rate is the maximum error rate as a percentage of the sample mean (sum and average will be accurate - count will be approximate)
    # read table
    # sample_size=1000
    print("here")
    table = pq.read_table(table)
    #table = table.select(column_set + output_set).to_pandas()
    table = table.to_pandas() # include all columns for now
    print(type(table))
    # find the different values in each column
    # columns is a list of lists of values on which to partition
    columns = get_columns(column_set, table, partition_size)
    # generate statistics for each partition
    # iterate over each partition
    # enumerate all possible partitions given columns
    print(columns)
    partition_lists = enumerate_partitions(columns, len(columns))
    print(partition_lists)

    # for each partition, generate a sample and save it
    
    print(table.head())
    part_coll = PartitionCollection()
    
    for partition in partition_lists:
        # TODO this is slow because we filter the entire table for each partition
        part = Partition(partition, column_set, output_set, table)
        
        sample = part.sample(error_rate)
        part_coll.add_partition(part)
        # add metadata to the sample
        #new_metadata = {'pop_count': str(part.row_count).encode('utf-8'), 'sample_count': str(sample.shape[0]).encode('utf-8')}
        sample_pq = pa.Table.from_pandas(sample, preserve_index=False)
        # existing_metadata = sample_pq.schema.metadata
        # updated_metadata = {**existing_metadata, **new_metadata} if existing_metadata else new_metadata
        #sample_pq = sample_pq.replace_schema_metadata(updated_metadata)
        with pa.OSFile("samples/" + part.name() + ".parquet", 'wb') as sink:
            with pa.parquet.ParquetWriter(sink, sample_pq.schema, compression='snappy', flavor='spark') as writer:
                writer.write_table(sample_pq)
    # save the partition collection
    part_coll.serialize("partitions.pkl")

def get_columns(column_set, table, partition_size):
    columns = [[] for i in range(len(column_set))]
    for i, column in enumerate(column_set):
        print(column)
        unique_values = table[column].unique()
        if len(unique_values) > 100:
            # divide range into 100 partitions
            # add the range of each partition to the list
            # convert ChunkedArray to pandas DataFrame
            column_data = table[column]
            
            # get range of values
            min_val = column_data.min()
            max_val = column_data.max()
            partition_range = (max_val - min_val) / partition_size
            columns[i] = [[min_val + partition_range * i, min_val + partition_range * (i +1)] for i in range(partition_size)]
        else:
            columns[i] = unique_values
        print(table[column].unique())
    return columns
   

    # for each column, generate a sample of size sample_size (to start - later to be informed by variance etc.)

    # if over 100 unique values, generate a variance optimal sample with 100 partitions



    # generate sample for each partition

    # save the sample in the samples folder


def main():
    table = 'db_parq/lineitem.parquet'
    column_set = ["l_returnflag", "l_extendedprice"]
    output_set = ["l_tax"]

    '''table = 'db_parq/lineitem.parquet'
    column_set = ["l_returnflag", "l_shipmode"]
    output_set = ["l_extendedprice"]'''
    sample_gen(column_set, output_set, table)
if __name__ == "__main__":
    main()