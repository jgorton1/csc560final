import pyarrow.parquet as pq
import pandas as pd
import numpy as np
import os
# a data structure to store statistics of a partition
class Partition:
    def __init__(self, parts, column_set, output_set, data):
        # parts is a list of values that define the partition - e.g. ["R", [0,100]]
        # column_set is a list of column names
        # data is a pyarrow table
        self.parts = parts
        self.column_set = column_set
        self.output_set = output_set
        if data.shape[1] != len(column_set) + len(output_set):
            raise ValueError("Data does not have the correct number of columns")
            #data = data.select(column_set + output_set)
        
        # get indexes of union of column_set and output_set
        self.data = data
    def sample(self, error_rate):
        print("yuh")
        # error_rate is the maximum error rate as a percentage of the sample mean of the output_set
        print(self.output_set[0])
        data = self.data[self.output_set[0]].astype(float)
        print(data.head())
        error = error_rate * data.mean()
        # for simplicity, 95% confidence interval (assuming normal distribution)
        S = data.std()
        print(S)
        N = data.shape[0]
        n_0 = 1.96**2 * S**2 / error**2
        n = n_0 / (1 + n_0 / N)
        sample_size =  min(int(n), N)
        print(sample_size)
        # sample the data
        return self.data.sample(sample_size)
    def name(self):
        # return a string that represents the partition
        return "_".join([str(part) for part in self.parts])

def generate_partitions(columns, column_set, data, n):
    # columns is a list of lists of values
    # eg columns = [["A", "B"],[[1,2],[2,3]]]
    # return a list of all possible partitions
    # eg [["A", [1,2]], ["A", [2,3]], ["B", [1,2]], ["B", [2,3]]
    if n == 1:
        ret_data = []
        for i, part in enumerate(columns[0]):
            if type(part) == str:  
                print("filter on str")
                data = data[data[column_set[i]] == part]
            elif type(part) == list:
                data = data[(data[column_set[i]] >= part[0])][(data[column_set[i]] < part[1])]
        return [[part] for part in columns[0]]
    else:
        partitions = []
        for part in columns[0]:
            for partition in generate_partitions(columns[1:], n-1):
                partitions.append([part] + partition)
        return partitions


def sample_gen(column_set, output_set, table, error_rate=0.03, partition_size=10):
    # column_set is a set of column names
    # table is name of parquet file to be read
    # error_rate is the maximum error rate as a percentage of the sample mean (sum and average will be accurate - count will be approximate)
    # read table
    # sample_size=1000
    print("here")
    table = pq.read_table(table)
    table = table.select(column_set + output_set).to_pandas()
    print(type(table))
    # find the different values in each column
    columns = get_columns(column_set, table, partition_size)
    # enumerate all possible partitions given columns
    print(columns)
    partitions, datas = generate_partitions(columns, len(columns))
    print(partitions)
    # for each partition, generate a sample and save it
    
    print(table.head())
    
    for partition in partitions:
        part = Partition(partition, column_set, output_set, table)
        sample = part.sample(error_rate)
        sample.to_parquet("samples/" + part.name() + ".parquet")
        # store this in a logfile so we don't recreate samples unnecessarily
        with open("logfile.txt", "a") as logfile:
            logfile.write(part.name() + "\n")

def get_columns(column_set, table, partition_size):
    columns = [[] for i in range(len(column_set))]
    for i, column in enumerate(column_set):
        print(column)
        unique_values = table[column].unique()
        if len(unique_values) > 100:
            # divide range into 100 partitions
            # add the range of each partition to the list
            column_data = table[column]
            # get range of values
            min_val = column_data.min()
            max_val = column_data.max()
            partition_range = (max_val - min_val) / partition_size
            columns[i] = [[min_val + partition_range * i, min_val + partition_range * (i +1)] for i in range(partition_size)]
        else:
            columns[i] = unique_values
        print(table[column].unique())
    return columns
   
    # for each column, generate a sample of size sample_size (to start - later to be informed by variance etc.)
    # if over 100 unique values, generate a variance optimal sample with 100 partitions
    # generate sample for each partition
    # save the sample in the samples folder

def main():
    table = 'db_parq/lineitem.parquet'
    column_set = ["l_returnflag", "l_extendedprice"]
    output_set = ["l_tax"]
    sample_gen(column_set, output_set, table)

if __name__ == "__main__":
    main()
    
